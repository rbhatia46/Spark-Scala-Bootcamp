{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://rinmac452.domain.name:4042\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1596283742866)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\n",
       "y: Int = 3\n",
       "res0: Int = 5\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Testing the interpreter\n",
    "val x = 2\n",
    "val y = 3\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6a3d82ac\n",
       "df: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading a dataframe\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Users/rahul.bhatia/Desktop/scala-training/CitiGroup2006_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Look at the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(755,6)\n"
     ]
    }
   ],
   "source": [
    "// Shape of the dataframe\n",
    "println(df.count(),df.columns.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Firsts  few rows of a dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|      Date|              Open|             High|               Low|             Close|           Volume|\n",
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|       755|               755|              755|               755|               755|              755|\n",
      "|   mean|      null| 386.0923178807949|390.6590596026489|380.80170860927143| 385.3421456953643|6308596.382781457|\n",
      "| stddev|      null|149.32301134820133|148.5151130063523|150.53136890891344|149.83310074439177| 8099892.56297633|\n",
      "|    min|2006-01-03|              54.4|             55.3|              30.5|              37.7|           632860|\n",
      "|    max|2008-12-31|             566.0|            570.0|             555.5|             564.1|        102869289|\n",
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Describe method\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "| High|  Low|Close|\n",
      "+-----+-----+-----+\n",
      "|493.8|481.1|492.9|\n",
      "|491.0|483.5|483.8|\n",
      "|487.8|484.0|486.2|\n",
      "|489.0|482.0|486.2|\n",
      "|487.4|483.0|483.9|\n",
      "|485.5|480.8|485.4|\n",
      "|495.8|485.8|489.8|\n",
      "|491.0|488.8|490.3|\n",
      "|491.9|487.3|489.2|\n",
      "|487.0|482.7|484.3|\n",
      "|486.7|481.1|483.8|\n",
      "|485.8|477.0|479.4|\n",
      "|474.0|456.3|456.9|\n",
      "|463.8|457.0|460.0|\n",
      "|463.6|459.9|460.1|\n",
      "|463.7|460.1|462.3|\n",
      "|475.5|464.5|470.1|\n",
      "|473.7|466.0|468.7|\n",
      "|469.9|466.6|468.2|\n",
      "|470.5|465.5|465.8|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Selecting multiple columsn from a dataframe\n",
    "df.select($\"High\",$\"Low\",$\"Close\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creating a new column\n",
    "val df2 = df.withColumn(\"High+Low\",df(\"High\") + df(\"Low\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- High+Low: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// See the schema of a dataframe\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              HPL|\n",
      "+-----------------+\n",
      "|974.9000000000001|\n",
      "|            974.5|\n",
      "|            971.8|\n",
      "|            971.0|\n",
      "|            970.4|\n",
      "|            966.3|\n",
      "|            981.6|\n",
      "|            979.8|\n",
      "|            979.2|\n",
      "|            969.7|\n",
      "|            967.8|\n",
      "|            962.8|\n",
      "|            930.3|\n",
      "|            920.8|\n",
      "|            923.5|\n",
      "|            923.8|\n",
      "|            940.0|\n",
      "|            939.7|\n",
      "|            936.5|\n",
      "|            936.0|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Renaming a column\n",
    "df2.select(df2(\"High+Low\").as(\"HPL\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic filter operations using Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import below to be able to use the Scala $ syntax\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Scala Notation\n",
    "df.filter($\"Close\">480).show(5)\n",
    "\n",
    "// SQL Notation\n",
    "df.filter(\"Close>480\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: Long = 325\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter($\"Close\">480).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-20|472.1|474.0|456.3|456.9|4781930|\n",
      "|2006-01-23|460.0|463.8|457.0|460.0|2025500|\n",
      "|2006-01-24|462.9|463.6|459.9|460.1|2083740|\n",
      "|2006-01-25|461.4|463.7|460.1|462.3|1591940|\n",
      "|2006-01-26|465.5|475.5|464.5|470.1|1988600|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-20|472.1|474.0|456.3|456.9|4781930|\n",
      "|2006-01-23|460.0|463.8|457.0|460.0|2025500|\n",
      "|2006-01-24|462.9|463.6|459.9|460.1|2083740|\n",
      "|2006-01-25|461.4|463.7|460.1|462.3|1591940|\n",
      "|2006-01-26|465.5|475.5|464.5|470.1|1988600|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Multiple Conditions Filter using Scala Notation\n",
    "df.filter($\"Close\"<480 && $\"High\"<480).show(5)\n",
    "\n",
    "// Multiple Conditions Filter using SQL Notation\n",
    "df.filter(\"Close<480 AND High < 480\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CH_Low: Array[org.apache.spark.sql.Row] = Array([2006-01-20,472.1,474.0,456.3,456.9,4781930], [2006-01-23,460.0,463.8,457.0,460.0,2025500], [2006-01-24,462.9,463.6,459.9,460.1,2083740], [2006-01-25,461.4,463.7,460.1,462.3,1591940], [2006-01-26,465.5,475.5,464.5,470.1,1988600], [2006-01-27,470.1,473.7,466.0,468.7,1412760], [2006-01-30,468.7,469.9,466.6,468.2,1057630], [2006-01-31,468.3,470.5,465.5,465.8,1887280], [2006-02-01,465.9,467.2,461.1,463.3,1844970], [2006-02-02,459.0,461.0,451.0,451.8,2325470], [2006-02-03,450.7,456.1,448.1,450.6,1666510], [2006-02-06,452.6,456.1,450.9,451.7,1147430], [2006-02-07,452.0,453.8,450.0,450.5,1207780], [2006-02-08,453.3,455.3,450.7,453.6,1051370], [2006-02-09,455.0,461.0,454.3,457.9,1357740], [2006-02-10,457.0,460.7,452.5,459.6,1272030], [2006-02-13,4...\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instead of showing the result everytime, we can store it and collect it in a new object.\n",
    "\n",
    "val CH_Low = df.filter(\"Close<480 AND High<480\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res67: org.apache.spark.sql.Row = [2006-03-23,475.8,479.8,474.7,477.7,1378500]\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Retroeve an element form the above array\n",
    "CH_Low(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-04-27|472.0|484.4|471.5|481.5|2464800|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-04-27|472.0|484.4|471.5|481.5|2464800|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Equals to doesn't work with \"==\" operator, use \"===\"\n",
    "df.filter($\"High\"===484.4).show()\n",
    "\n",
    "// SQL \"=\" Notation just works fine\n",
    "df.filter(\"High = 484.40\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important functions\n",
    "\n",
    "List of all available functions - https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Pearson Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   corr(High, Low)|\n",
      "+------------------+\n",
      "|0.9992999172726325|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"High\",\"Low\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

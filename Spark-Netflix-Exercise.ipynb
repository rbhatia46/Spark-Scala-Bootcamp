{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@64db79c\n",
       "df: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Users/rahul.bhatia/Desktop/scala-training/Netflix_2011_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+----------+-----------------+---------+------------------+\n",
      "|      Date|             Open|              High|       Low|            Close|   Volume|         Adj Close|\n",
      "+----------+-----------------+------------------+----------+-----------------+---------+------------------+\n",
      "|2011-10-24|       119.100002|120.28000300000001|115.100004|       118.839996|120460200|         16.977142|\n",
      "|2011-10-25|        74.899999|         79.390001| 74.249997|        77.370002|315541800|11.052857000000001|\n",
      "|2011-10-26|            78.73|         81.420001| 75.399997|        79.400002|148733900|         11.342857|\n",
      "|2011-10-27|        82.179998| 82.71999699999999| 79.249998|80.86000200000001| 71190000|11.551428999999999|\n",
      "|2011-10-28|        80.280002|         84.660002| 79.599999|84.14000300000001| 57769600|             12.02|\n",
      "|2011-10-31|83.63999799999999|         84.090002| 81.450002|        82.080003| 39653600|         11.725715|\n",
      "|2011-11-01|        80.109998|         80.999998|     78.74|        80.089997| 33016200|         11.441428|\n",
      "|2011-11-02|        80.709998|         84.400002| 80.109998|        83.389999| 41384000|         11.912857|\n",
      "|2011-11-03|        84.130003|         92.600003| 81.800003|        92.290003| 94685500|13.184285999999998|\n",
      "|2011-11-04|91.46999699999999| 92.89000300000001| 87.749999|        90.019998| 84483700|             12.86|\n",
      "|2011-11-07|             91.0|         93.839998| 89.979997|        90.830003| 47485200|         12.975715|\n",
      "|2011-11-08|91.22999899999999|         92.600003| 89.650002|        90.470001| 31906000|         12.924286|\n",
      "|2011-11-09|        89.000001|         90.440001| 87.999998|        88.049999| 28756000|         12.578571|\n",
      "|2011-11-10|        89.290001| 90.29999699999999| 84.839999|85.11999899999999| 39614400|             12.16|\n",
      "|2011-11-11|        85.899997|         87.949997|      83.7|        87.749999| 38140200|         12.535714|\n",
      "|2011-11-14|        87.989998|              88.1|     85.45|        85.719999| 21811300|         12.245714|\n",
      "|2011-11-15|            85.15|         87.050003| 84.499998|        86.279999| 21372400|         12.325714|\n",
      "|2011-11-16|        86.460003|         86.460003| 80.890002|        81.180002| 34560400|11.597142999999999|\n",
      "|2011-11-17|            80.77|         80.999998| 75.789999|        76.460001| 52823400|         10.922857|\n",
      "|2011-11-18|             76.7|         78.999999| 76.039998|        78.059998| 34729100|         11.151428|\n",
      "+----------+-----------------+------------------+----------+-----------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a new dataframe with a column called HV Ratio that\n",
    "// is the ratio of the High Price versus volume of stock traded\n",
    "// for a day.\n",
    "val df2 = df.withColumn(\"HV Ratio\",df(\"High\")/df(\"Volume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------+----------+----------+--------+------------------+\n",
      "|      Date|             Open|      High|       Low|     Close|  Volume|         Adj Close|\n",
      "+----------+-----------------+----------+----------+----------+--------+------------------+\n",
      "|2015-07-13|686.6900019999999|716.159996|686.550026|707.610001|33205200|101.08714300000001|\n",
      "+----------+-----------------+----------+----------+----------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What day had the Peak High in Price?\n",
    "df.orderBy($\"High\".desc).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      avg(Close)|\n",
      "+----------------+\n",
      "|230.522453845909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What is the mean of the Close column?\n",
    "df.select(mean(\"Close\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Volume)|\n",
      "+-----------+\n",
      "|  315541800|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(Volume)|\n",
      "+-----------+\n",
      "|    3531300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What is the max and min of the Volume column?\n",
    "df.select(max(\"Volume\")).show()\n",
    "df.select(min(\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For Scala/Spark $ Syntax\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: Long = 1218\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How many days was the Close lower than $ 600?\n",
    "df.filter($\"Close\"<600).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: Double = 4.924543288324067\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// What percentage of the time was the High greater than $500 ?\n",
    "(df.filter($\"High\">500).count()*1.0/df.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  corr(High, Volume)|\n",
      "+--------------------+\n",
      "|-0.20960233287942157|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What is the Pearson correlation between High and Volume?\n",
    "df.select(corr(\"High\",\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|         max(High)|\n",
      "+----+------------------+\n",
      "|2015|        716.159996|\n",
      "|2013|        389.159988|\n",
      "|2014|        489.290024|\n",
      "|2012|        133.429996|\n",
      "|2016|129.28999299999998|\n",
      "|2011|120.28000300000001|\n",
      "+----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "yeardf: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 6 more fields]\n",
       "yearmaxs: org.apache.spark.sql.DataFrame = [Year: int, max(Year): int ... 1 more field]\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// What is the max High per year?\n",
    "val yeardf = df.withColumn(\"Year\",year(df(\"Date\")))\n",
    "val yearmaxs = yeardf.select($\"Year\",$\"High\").groupBy(\"Year\").max()\n",
    "yearmaxs.select($\"Year\",$\"max(High)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|        avg(Close)|\n",
      "+-----+------------------+\n",
      "|   12| 199.3700942358491|\n",
      "|    1|212.22613874257422|\n",
      "|    6| 295.1597153490566|\n",
      "|    3| 249.5825228971963|\n",
      "|    5|264.37037614150944|\n",
      "|    9|206.09598121568627|\n",
      "|    4|246.97514271428562|\n",
      "|    8|195.25599892727263|\n",
      "|    7|243.64747528037387|\n",
      "|   10|205.93297300900903|\n",
      "|   11| 194.3172275445545|\n",
      "|    2| 254.1954634020619|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "monthdf: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 6 more fields]\n",
       "monthavgs: org.apache.spark.sql.DataFrame = [Month: int, avg(Month): double ... 1 more field]\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// What is the average Close for each Calender Month?\n",
    "val monthdf = df.withColumn(\"Month\",month(df(\"Date\")))\n",
    "val monthavgs = monthdf.select($\"Month\",$\"Close\").groupBy(\"Month\").mean()\n",
    "monthavgs.select($\"Month\",$\"avg(Close)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
